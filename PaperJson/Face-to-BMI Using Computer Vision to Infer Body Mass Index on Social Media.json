{
    "Title": "Introduction",
    "Author": "",
    "Subject": "",
    "KeyWords": [],
    "Outlines": [
        {
            "index": 0,
            "name": "Prologue",
            "type": "prol",
            "level": 4,
            "content": "## Face-to-BMI: Using Computer Vision to Infer Body Mass Index on Social Media∗ ## Enes Kocabey∗, Mustafa Camurcu†, Ferda Ofli‡, Yusuf Aytar∗, Javier Marin∗, Antonio Torralba∗, Ingmar Weber‡ ∗MIT-CSAIL,†Northeastern University,‡Qatar Computing Research Institute, HBKU ∗{kocabey,yaytar,jmarin,torralba}@mit.edu,†camurcu.m@husky.neu.edu,‡{fofli,iweber}@hbku.edu.qa Abstract A person’s weight status can have profound implications on their life, ranging from mental health, to longevity, to finan- cial income. At the societal level, “fat shaming” and other forms of “sizeism” are a growing concern, while increasing obesity rates are linked to ever raising healthcare costs. For these reasons, researchers from a variety of backgrounds are interested in studying obesity from all angles. To obtain data, traditionally, a person would have to accurately self-report their body-mass index (BMI) or would have to see a doctor to have it measured. In this paper, we show how computer vision can be used to infer a person’s BMI from social me- dia images. We hope that our tool, which we release, helps to advance the study of social aspects related to body weight. ## Introduction Together with a person’s gender, age and race, their weight status is a publicly visible signal that can have profound in- fluence on many aspects of their life. Most obviously, it can affect their health as having a larger BMI is linked to an in- creased risk of both cardio-vascular diseases and diabetes, though not necessarily in a straight-forward manner (Meigs and others 2006). However, other aspects of the burden im- posed by obesity come in the form of “fat shaming” and other forms of “sizeism”. For example, obesity is related to a lower income^1 and part of the reason seems to be due to weight-based discrimination (Puhl and others 2008). Even among health professionals “sizeism” is so prevalent that it has become a health hazard as, when faced with overweight patients, care providers stop to look for alternative explana- tions for a medical condition (Chrisler and Barney 2016). For these reasons, researchers from a variety of back- grounds are interested in studying obesity from all angles. To obtain data, traditionally, a person would have to accurately self-report their body-mass index (BMI) or would have to see a doctor to have it measured. In this paper, we propose a new pipeline using state-of-the-art computer vision tech- niques to infer a person’s BMI from social media images, ∗This is a preprint of a short paper accepted at ICWSM’17. Please cite that version instead. (^1) [http://www.forbes.com/sites/](http://www.forbes.com/sites/) freekvermeulen/2011/03/22/the-price-of- obesity-how-your-salary-depends-on-your- weight/ such as their profile picture. We show that the performance in distinguishing for a given pair the more overweight per- son is similar to human performance. ## ",
            "detail": ""
        },
        {
            "index": 1,
            "name": "Related Work",
            "type": "sect",
            "level": 4,
            "content": "Being overweight can lead to a range of negative conse-quences with the most direct ones concerning health. Giventhis importance, recent studies in psychology and sociologyinvestigate how humans perceive health from profile pic-tures. (Coetzee and others 2009) showed that facial adiposity(i.e., perception of weight in the face) was a significant pre-dictor of the perceived health. Furthermore, they showed thatperceived facial adiposity was significantly associated withcardiovascular health and reported infections, and hence, animportant and valid cue to actual health. In a similar study,(Henderson and others 2016) explored the effect of a varietyof facial characteristics on humans’ health judgment. Theyfound that facial features such as skin yellowness, mouthcurvature and shape were correlated positively whereas fa-cial shape associated with adiposity was correlated nega-tively with impression of health.In light of these studies, (Weber and Mejova 2016) took acrowdsourcing approach to understand health judgments ofhumans in a body-weight-inference task from profile pic-tures. However, since the judgment of whether a picture“is overweight” is a rather subjective task, their work suf-fered from the bias of human annotators to falsely equate“overweight” with “abnormal.” To eliminate such limita-tions, (Wen and Guo 2013) showed that it is indeed feasibleto some degree to predict BMI from face images automati-cally using computational techniques. Their approach reliedon detecting a number of fiducial points in each face im-age and computing hand-crafted geometric facial features totrain a regression model for BMI prediction. Their dataset,however, comprised exclusively passport-style frontal facephotos with clean background, and hence, the performanceof their BMI prediction model is uncertain for noisy socialmedia pictures.##",
            "detail": ""
        },
        {
            "index": 2,
            "name": "Faces with BMI Data",
            "type": "sect",
            "level": 4,
            "content": "To ensure that our system works with noisy, often low qual-ity social media pictures, such as profile pictures, we used arXiv:1703.03156v1 [cs.HC] 9 Mar 2017the set of annotated images from the VisualBMI project^2.These images are, in turn, collected from Reddit posts thatlink to the imgur.com service. Examples of the underly-ing Reddit posts can be found in the “progresspics” sub-Reddit^3. The VisualBMI dataset comprises a total of 16,images containing a pair of “before” and “after” images, an-notated with gender, height and previous and current bodyweights. We manually went through all of the image URLs,and cropped the faces. We ignored all the images except theones with two faces, since we only had previous and cur-rent body weights. After the manual cleaning process, wewere left with 2103 pairs of faces, with corresponding gen-der, height and previous and current body weights. Then foreach pair, we computed the previous BMI and current BMI.The BMI is defined as (body mass in kg) / (body height inm)^2.This led to a total of 4206 faces with corresponding gen-der and BMI information. Of these, seven were in the under-weight range (16<BMI≤18.5), 680 were normal (18.5<BMI≤25), 1151 were overweight (25<BMI≤30), 941were moderately obese (30<BMI≤35), 681 were severelyobese (35<BMI≤40) and 746 were very severely obese(40<BMI). The dataset contained 2438 males and 1768 fe-males. Figure 1 shows a selection of the faces that were usedfor training and evaluating our system.Figure 1: Examples of the cleaned images used for modeltraining. The black bars have been added to respect user pri-vacy, but the model is learned on the original, public images.##",
            "detail": ""
        },
        {
            "index": 3,
            "name": "Face-to-BMI System",
            "type": "sect",
            "level": 4,
            "content": "In this section, we describe our Face-to-BMI system start-ing with (i) the computer vision architecture used forbuilding the prediction model, and then (ii) the details ofthe evaluation and comparison with human performance.Our pre-trained models and scripts for using them can bedownloaded for academic research purposes athttp://face2bmi.csail.mit.edu.(^2) [http://www.visualbmi.com/](http://www.visualbmi.com/)(^3) https://www.reddit.com/r/progresspics/###",
            "detail": ""
        },
        {
            "index": 4,
            "name": "Computer Vision Architecture",
            "type": "sect",
            "level": 4,
            "content": "Many computer vision tasks have greatly benefited fromthe recent advances in deep learning (Parkhi and others2015; Krizhevsky and others 2012; Simonyan and Zisser-man 2014) and here we also utilize such models for the Face-to-BMI problem. The features learned in deep convolutionalnetworks are proven to be transferable and quite effectivewhen used in other visual recognition tasks (Yosinski andothers 2014; Girshick and others 2014), particularly whentraining samples are limited and learning a successful deepmodel is not feasible due to overfitting. For instance, (Ozbu-lak and others 2016) shows the success of this transfer forage and gender recognition tasks performed on face images.Considering that we also have limited training examples, weadopted a transfer learning approach.Our BMI prediction system is composed of two stages: (i)deep feature extraction, and (ii) training a regression model.For feature extraction we use two well-known deep models,one trained on general object classification (i.e., VGG-Net(Simonyan and Zisserman 2014)) and the other trained ona face recognition task (i.e., VGG-Face (Parkhi and others2015)). Both of these models are deep convolutional mod-els with millions of parameters, and trained on millions ofimages. The features from thefc6layer are extracted foreach face image in our training set. For the BMI regression,we use epsilon support vector regression models (Smola andVapnik 1997) due to its robust generalization behavior. Themodels are trained on the 3368 training images and tested on838 test images from the VisualBMI dataset. We make surethat the same individual does not exist in both training andtest sets (e.g., before and after images). The performance ofboth of our models are shown in Table 1. As also pointed outby (Ozbulak and others 2016), the features extracted from amore relevant model, i.e., VGG-Face, perform better com-pared to the VGG-Net features. Due to its superiority weuse VGG-Face features in our Face-to-BMI system.To see if our system could also be used for tracking weightchanges for a single person, rather than comparing acrosspeople, we also defined a different train-test split. We firstrandomly selected 838 unique individuals from our dataset.For each of these individuals, we randomly selected one ofthe two corresponding before-after face photos and addedthem to a new test set. All the remaining pictures were addedto the new training set. In this way, every person that has aface image in test set also had a face image in the trainingset. We kept the training and test sizes the same to ensure afair comparison. Our model achieved 0.68 correlation, com-pared to 0.65 in the across-people setup, suggesting that oursystem benefits from having a history of images to train onfor the individual it is making a prediction for.###",
            "detail": ""
        },
        {
            "index": 5,
            "name": "Human Evaluation",
            "type": "sect",
            "level": 4,
            "content": "We conduct a simple experiment to compare our Face-to-BMI system’s performance to that of humans. Given faceimages for two individuals, each “contestant,” i.e., machineand human, is required to tell which one is more overweight.Note that our system was not trained for this specific binaryclassification task though and a dedicated system might per-form better.Model Male Female OverallFace-to-BMI – VGG-Net 0.58 0.36 0.Face-to-BMI – VGG-Face 0.71 0.57 0.Table 1: The Pearsonrcorrelations on the test set for theBMI prediction task, broken down by gender. Note thatVGG-Face features yield a much better performance.For evaluation, we collect a total number of 900 pairs.This is obtained by using only samples coming from the testset, chosen such that pairs are equally distributed in gen-der subcategories (‘male vs. male’, ‘female vs. female’ and‘female vs. male’) and BMI difference between individualswithin a pair. Concretely, we randomly collect 300 ‘malevs. male’ pairs consisting in 15 subsets of 20-pairs each,such that, for each subsetSi, withi∈ { 0 ,... , 14 }, all pairs(a, b)∈Sisatisfy:(0.5 +i)<|BMIa−BMIb|≤(1.5 +i)We also collect 300 ‘female vs. female’ and ‘female vs.male’ pairs following the same strategy. In the latter case,we also assure that half of the pairs correspond to males be-ing more overweight and vice-versa. Furthermore, we try tobalance the overall BMI distribution across the whole spec-trum, from thin to obese.On the human side, we perform the aforementioned ques-tionnaire through Amazon Mechanical Turk^4. Each questionis shown to three unique users gathering a total of 2700 an-swers. The human performance is then obtained using allthe answers together and represented as the accuracy. Wedid not apply a majority voting approach as we wanted toevaluate the performance of anindividualhuman. On the ma-chine side, for each question we compare the system outputof each individual included in the pair to obtain an answer.Figure 2 depicts the results of comparison broken downby the pair’s gender type and the absolute BMI differencebetween individuals of each pair. The overall performancedifference between human and machine is less than2%, butwhen looking at different gender subcategories, there is abigger gap the ‘male vs. male’ comparisons,∼5%. Humansslightly outperform the machine for small BMI differences,and there is almost no performance difference for larger BMIdifferences.##",
            "detail": ""
        },
        {
            "index": 6,
            "name": "Discussion",
            "type": "sect",
            "level": 4,
            "content": "###",
            "detail": ""
        },
        {
            "index": 7,
            "name": "Algorithmic Bias",
            "type": "sect",
            "level": 4,
            "content": "As demographic groups differ in their BMI distribution, it islikely that, e.g., the race and the BMI are not independentattributes in the visualBMI data. This could then mean thatour system perpetuates existing stereotypes. For example,as African Americans have higher obesity rates in the USpopulation, an automated system might learn a prior proba-bility that increases the likelihood of a person to be labeledas obese simply based on their race.(^4) [http://mturk.com](http://mturk.com)Figure 2: Human vs. Face-to-BMI comparison broken downby gender and absolute BMI difference between individualsof each pair.To test if our system is biased in outputting a higher BMIfor a picture solely due to the person’s gender or race, wepaired users with a similar BMI, i.e., difference<1.0, but adifferent gender or race. Furthermore, these pairs were con-structed such that, in aggregate, each demographic grouphad the same number of (slightly) higher BMIs. For pairswith such close BMIs, an unbiased tool should pick mem-bers of either group 50% of the time. Hence we check if ourtool creates a distribution in the output that differs statisti-cally significantly from 50-50.For 2000 male-female pairs from the test set the tool pre-dicted a higher BMI for females in 1037 cases,p=. 05.Though the evidence is inconclusive, it is possible that oursystem is slightly biased against females. To test for racialbias we did not have a sufficient number of pairs in the testset and hence had to include examples from the training set.For 2000 White-African American pairs, our tool predicteda higher BMI for Whites in 1085 cases,p <. 05 , hinting ata small bias against Whites.###",
            "detail": ""
        },
        {
            "index": 8,
            "name": "Ethical Considerations",
            "type": "sect",
            "level": 4,
            "content": "Historically, 19th century phrenology^5 studied the potentiallink between the shape of skull and moral attributes, apply-ing a pseudo-scientific methodology to justify racism. Re-cently, researchers in China have started to predict if a facebelongs to a criminal which, they claim, they can detect withbetter-than-random accuracy. However, their work is largelybeing viewed both as unethical and as scientifically flawed(Biddle 2016).Going beyond moral attributes and the shape of the skull,psychologists have indeed shown that using only facial in-formation it is possible for a person to perform better thanrandom chance at guessing another’s personality (Little andPerrett 2007), an observation at the heart of physiognomy^6.Over the last couple of years, there has been a growing body(^5) https://en.wikipedia.org/wiki/Phrenology(^6) https://en.wikipedia.org/wiki/Physiognomyof work that successfully applies computer vision techniquesto automatically infer a person’s personality in particularfrom images shared on social media (Nie and others 2016;Dhall and Hoey 2016; Guntuku and others 2015; Liu andothers 2016).Most of the methods mentioned above work “better thanrandom guessing” but, when applied to a single individual,are still highly unreliable. This is partly because conceptssuch as personality are inherently vague and partly becausethe connection to facial features is weak.This caveat also largely applies to our tool which, despitea performance similar to humans, is still noisy at the indi-vidual level. However, at thepopulationlevel it can be usedto detect relative trends as long as it is not biased in a sys-tematic way. In other words, result of the form “the aver-age BMI for group X is larger than for group Y” are farmore robust than results of the form “this individual fromgroup X has a higher BMI than this other individual fromgroup Y”. This distinction also applies to the BMI whichis useful for studying population health but has shortcom-ings when used as a tool for individual health (Daniels 2009;Prentice and Jebb 2001).##",
            "detail": ""
        },
        {
            "index": 9,
            "name": "Conclusions",
            "type": "sect",
            "level": 4,
            "content": "In this work, we apply the most recent computer vision tech-niques to obtain a novel Face-to-BMI system. The perfor-mance of this tool is on par to that of humans for distinguish-ing the more overweight person when presented with a pairof profile images. We discuss issues related to algorithmicbias and ethical considerations when inferring informationfrom a person’s profile image. To limit the potential of abusewhile allowing others to replicate and build on our results,we make our pre-trained models only available to academicresearchers after describing the intended use.In future work, we will apply our method to social mediaprofile pictures to model population-level obesity rates. Pre-liminary results show that both regional and demographicdifferences in BMI are reflected in large amounts of Insta-gram profile pictures.## References[Biddle 2016] Biddle, S. 2016. Troubling studysays artificial intelligence can predict who willbe criminals based on facial features. The Inter-cept. https://theintercept.com/2016/11/18/troubling-study-says-artificial-intelligence-can-predict-who-will-be-criminals-based-on-facial-features/.[Chrisler and Barney 2016] Chrisler, J. C., and Barney, A.2016. Sizeism is a health hazard.Fat Studies0(0):1–16.[Coetzee and others 2009] Coetzee, V., et al. 2009. Facialadiposity: A cue to health?Perception38(11):1700–1711.[Daniels 2009] Daniels, S. R. 2009. The use of bmi in theclinical setting.Pediatrics124(Supplement 1):S35–S41.[Dhall and Hoey 2016] Dhall, A., and Hoey, J. 2016. Firstimpressions - predicting user personality from twitter profileimages. InHBU, 148–158.[Girshick and others 2014] Girshick, R., et al. 2014. Richfeature hierarchies for accurate object detection and seman-tic segmentation. InCVPR, 580–587.[Guntuku and others 2015] Guntuku, S. C., et al. 2015. Doothers perceive you as you want them to?: Modeling person-ality based on selfies. InASM, 21–26.[Henderson and others 2016] Henderson, A. J., et al. 2016.Perception of health from facial cues.Philosophical Trans-actions of the Royal Society of London B: Biological Sci-ences371(1693).[Krizhevsky and others 2012] Krizhevsky, A., et al. 2012.Imagenet classification with deep convolutional neural net-works. InNIPS, 1097–1105.[Little and Perrett 2007] Little, A. C., and Perrett, D. I. 2007.Using composite images to assess accuracy in personality at-tribution to faces.British Journal of Psychology98(1):111–126.[Liu and others 2016] Liu, L., et al. 2016. Analyzing person-ality through social media profile picture choice. InICWSM,211–220.[Meigs and others 2006] Meigs, J. B., et al. 2006. Body massindex, metabolic syndrome, and risk of type 2 diabetes orcardiovascular disease.The Journal of Clinical Endocrinol-ogy & Metabolism91(8):2906–2912.[Nie and others 2016] Nie, J., et al. 2016. Social media pro-filer: Inferring your social media personality from visual at-tributes in portrait. InPCM, 640–649.[Ozbulak and others 2016] Ozbulak, G., et al. 2016. Howtransferable are cnn-based features for age and gender clas-sification? InBIOSIG, 1–6.[Parkhi and others 2015] Parkhi, O. M., et al. 2015. Deepface recognition. InBritish Machine Vision Conference.[Prentice and Jebb 2001] Prentice, A. M., and Jebb, S. A.2001. Beyond body mass index.Obesity reviews2(3):141–147.[Puhl and others 2008] Puhl, R. M., et al. 2008. Perceptionsof weight discrimination: prevalence and comparison to raceand gender discrimination in america. Int J Obes32:992–1000.[Simonyan and Zisserman 2014] Simonyan, K., and Zisser-man, A. 2014. Very deep convolutional networks for large-scale image recognition.arXiv preprint arXiv:1409.1556.[Smola and Vapnik 1997] Smola, A., and Vapnik, V. 1997.Support vector regression machines.NIPS9:155–161.[Weber and Mejova 2016] Weber, I., and Mejova, Y. 2016.Crowdsourcing health labels: Inferring body weight fromprofile pictures. InDH, 105–109.[Wen and Guo 2013] Wen, L., and Guo, G. 2013. A compu-tational approach to body mass index prediction from faceimages.Image and Vision Computing31:392–400.[Yosinski and others 2014] Yosinski, J., et al. 2014. Howtransferable are features in deep neural networks? InNIPS,3320–3328.",
            "detail": ""
        }
    ],
    "Text": "Prologue\n\nRelated Work\n\nFaces with BMI Data\n\nFace-to-BMI System\n\nComputer Vision Architecture\n\nHuman Evaluation\n\nDiscussion\n\nAlgorithmic Bias\n\nEthical Considerations\n\nConclusions\n\n",
    "Date": "2017-03-10",
    "HasInfo": true,
    "HasOLF": true,
    "ForceSplit": false
}